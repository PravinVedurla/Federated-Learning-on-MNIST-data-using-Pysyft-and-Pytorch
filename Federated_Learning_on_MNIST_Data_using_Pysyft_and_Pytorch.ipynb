{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated Learning on MNIST Data using Pysyft and Pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PravinVedurla/Federated-Learning-on-MNIST-data-using-Pysyft-and-Pytorch/blob/master/Federated_Learning_on_MNIST_Data_using_Pysyft_and_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkSNX2nSEOZq",
        "colab_type": "text"
      },
      "source": [
        "<h2>Installing Syft and then importing Torch with required dependencies</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX7gxtDwCfUm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "10f2b477-cd03-4011-c138-00c01a5e407d"
      },
      "source": [
        "!pip install syft\n",
        "!pip install --upgrade --force-reinstall zstd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syft in /usr/local/lib/python3.6/dist-packages (0.1.21a1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.16.4)\n",
            "Requirement already satisfied: websockets>=7.0 in /usr/local/lib/python3.6/dist-packages (from syft) (8.0)\n",
            "Requirement already satisfied: websocket-client>=0.56.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.56.0)\n",
            "Requirement already satisfied: flask-socketio>=3.3.2 in /usr/local/lib/python3.6/dist-packages (from syft) (4.1.0)\n",
            "Requirement already satisfied: tf-encrypted>=0.5.4 in /usr/local/lib/python3.6/dist-packages (from syft) (0.5.6)\n",
            "Requirement already satisfied: torchvision>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.3.0)\n",
            "Requirement already satisfied: lz4>=2.1.6 in /usr/local/lib/python3.6/dist-packages (from syft) (2.1.10)\n",
            "Requirement already satisfied: msgpack>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from syft) (0.6.1)\n",
            "Requirement already satisfied: tblib>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from syft) (1.1.0)\n",
            "Requirement already satisfied: zstd>=1.4.0.0 in /usr/local/lib/python3.6/dist-packages (from syft) (1.4.0.0)\n",
            "Requirement already satisfied: Flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from syft) (1.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from syft) (0.21.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.56.0->syft) (1.12.0)\n",
            "Requirement already satisfied: python-socketio>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from flask-socketio>=3.3.2->syft) (4.2.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (5.1.1)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.3.0->syft) (4.3.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (7.0)\n",
            "Requirement already satisfied: Werkzeug>=0.14 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (0.15.4)\n",
            "Requirement already satisfied: Jinja2>=2.10 in /usr/local/lib/python3.6/dist-packages (from Flask>=1.0.2->syft) (2.10.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->syft) (0.13.2)\n",
            "Requirement already satisfied: python-engineio>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from python-socketio>=2.1.0->flask-socketio>=3.3.2->syft) (3.8.2.post1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.33.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.7.1)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (0.1.7)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.7.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.3.0->syft) (0.46)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10->Flask>=1.0.2->syft) (1.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted>=0.5.4->syft) (41.0.1)\n",
            "Collecting zstd\n",
            "Installing collected packages: zstd\n",
            "  Found existing installation: zstd 1.4.0.0\n",
            "    Uninstalling zstd-1.4.0.0:\n",
            "      Successfully uninstalled zstd-1.4.0.0\n",
            "Successfully installed zstd-1.4.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hxzFITTDuu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms \n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.set_default_tensor_type(torch.cuda.FloatTensor)  # To tackle an error with pysyft not accepting model transfer to cuda."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BixiSRGzEjY-",
        "colab_type": "text"
      },
      "source": [
        "## Creating a torch hook to be used along with syft."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPHti0M2ENbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "cb9ce606-63db-4674-9665-eae8d67351c9"
      },
      "source": [
        "import syft as sy\n",
        "hook = sy.TorchHook(torch)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0712 15:58:38.994253 139795915089792 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/usr/local/lib/python3.6/dist-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
            "W0712 15:58:39.010136 139795915089792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn2aLro0GZ-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating workers to distribute data to.\n",
        "\n",
        "bob = sy.VirtualWorker(hook, id='bob')\n",
        "alice = sy.VirtualWorker(hook, id='alice')\n",
        "jon = sy.VirtualWorker(hook, id='jon')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJkGcgRBGj_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Some parameters defined here\n",
        "\n",
        "batch_size = 64\n",
        "test_batch_size = 1000\n",
        "epochs = 10\n",
        "log_interval = 30\n",
        "save_model = False\n",
        "no_cuda = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kWa8S04Hp1x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6809dcfb-f799-4a68-9e0f-40a5323540bd"
      },
      "source": [
        "use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiWllyHxQIg6",
        "colab_type": "text"
      },
      "source": [
        "# Loading Data and distributing amongst workers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB2QLE_uNvM2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining the transform for both train and test loader.\n",
        "transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])\n",
        "\n",
        "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
        "    datasets.MNIST('../data', train=True, download=True, transform=transform).federate((bob, alice, jon)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transform), batch_size=test_batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY9RXC5SNNUC",
        "colab_type": "text"
      },
      "source": [
        "# Building the network itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r66Xii_H97S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Forward Convolutional Neural Network Architecture model\n",
        "    \n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBOwQ-BNu3gj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8edf3acb-26d7-43ce-95af-11baad5357c3"
      },
      "source": [
        "model = Classifier()\n",
        "model = model.to(device)  #pushing the model into available device.\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(federated_train_loader): # iterate through each worker's dataset\n",
        "        \n",
        "        model.send(data.location) #send the model to the right location ; data.location returns the worker name in which the data is present\n",
        "        \n",
        "        data, target = data.to(device), target.to(device) # pushing both the data and target labels onto the available device.\n",
        "        \n",
        "        optimizer.zero_grad() # 1) erase previous gradients (if they exist)\n",
        "        output = model(data)  # 2) make a prediction\n",
        "        loss = F.nll_loss(output, target)  # 3) calculate how much we missed\n",
        "        loss.backward()  # 4) figure out which weights caused us to miss\n",
        "        optimizer.step()  # 5) change those weights\n",
        "        model.get()  # get the model back (with gradients)\n",
        "        \n",
        "        if batch_idx % log_interval == 0:\n",
        "            loss = loss.get() #get the loss back\n",
        "            print('Epoch: {} [Training: {:.0f}%]\\tLoss: {:.6f}'.format(epoch, 100. * batch_idx / len(federated_train_loader), loss.item()))\n",
        "    \n",
        "    #Testing phase\n",
        "    \n",
        "    model.eval()  \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            \n",
        "            data, target = data.to(device), target.to(device) \n",
        "            output = model(data) # Getting a prediction\n",
        "            \n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() #updating test loss\n",
        "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
        "            \n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() #correct pred in the current test set.\n",
        "\n",
        "    test_loss /= len(test_loader.dataset) \n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "torch.save(model.state_dict(), \"mnist_cnn.pt\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 [Training: 0%]\tLoss: 2.318598\n",
            "Epoch: 1 [Training: 3%]\tLoss: 2.211987\n",
            "Epoch: 1 [Training: 6%]\tLoss: 2.058846\n",
            "Epoch: 1 [Training: 10%]\tLoss: 1.601909\n",
            "Epoch: 1 [Training: 13%]\tLoss: 1.053527\n",
            "Epoch: 1 [Training: 16%]\tLoss: 0.682115\n",
            "Epoch: 1 [Training: 19%]\tLoss: 0.708618\n",
            "Epoch: 1 [Training: 22%]\tLoss: 0.464039\n",
            "Epoch: 1 [Training: 26%]\tLoss: 0.417994\n",
            "Epoch: 1 [Training: 29%]\tLoss: 0.473361\n",
            "Epoch: 1 [Training: 32%]\tLoss: 0.442335\n",
            "Epoch: 1 [Training: 35%]\tLoss: 0.357673\n",
            "Epoch: 1 [Training: 38%]\tLoss: 0.247065\n",
            "Epoch: 1 [Training: 42%]\tLoss: 0.254549\n",
            "Epoch: 1 [Training: 45%]\tLoss: 0.479151\n",
            "Epoch: 1 [Training: 48%]\tLoss: 0.207273\n",
            "Epoch: 1 [Training: 51%]\tLoss: 0.194028\n",
            "Epoch: 1 [Training: 54%]\tLoss: 0.233580\n",
            "Epoch: 1 [Training: 58%]\tLoss: 0.214929\n",
            "Epoch: 1 [Training: 61%]\tLoss: 0.274084\n",
            "Epoch: 1 [Training: 64%]\tLoss: 0.213623\n",
            "Epoch: 1 [Training: 67%]\tLoss: 0.165152\n",
            "Epoch: 1 [Training: 70%]\tLoss: 0.241492\n",
            "Epoch: 1 [Training: 74%]\tLoss: 0.335058\n",
            "Epoch: 1 [Training: 77%]\tLoss: 0.171820\n",
            "Epoch: 1 [Training: 80%]\tLoss: 0.054685\n",
            "Epoch: 1 [Training: 83%]\tLoss: 0.218398\n",
            "Epoch: 1 [Training: 86%]\tLoss: 0.217178\n",
            "Epoch: 1 [Training: 90%]\tLoss: 0.249625\n",
            "Epoch: 1 [Training: 93%]\tLoss: 0.137095\n",
            "Epoch: 1 [Training: 96%]\tLoss: 0.158469\n",
            "Epoch: 1 [Training: 99%]\tLoss: 0.255234\n",
            "\n",
            "Test set: Average loss: 0.1746, Accuracy: 9476/10000 (95%)\n",
            "\n",
            "Epoch: 2 [Training: 0%]\tLoss: 0.112819\n",
            "Epoch: 2 [Training: 3%]\tLoss: 0.090772\n",
            "Epoch: 2 [Training: 6%]\tLoss: 0.233704\n",
            "Epoch: 2 [Training: 10%]\tLoss: 0.081826\n",
            "Epoch: 2 [Training: 13%]\tLoss: 0.124007\n",
            "Epoch: 2 [Training: 16%]\tLoss: 0.142451\n",
            "Epoch: 2 [Training: 19%]\tLoss: 0.114108\n",
            "Epoch: 2 [Training: 22%]\tLoss: 0.177630\n",
            "Epoch: 2 [Training: 26%]\tLoss: 0.049652\n",
            "Epoch: 2 [Training: 29%]\tLoss: 0.167364\n",
            "Epoch: 2 [Training: 32%]\tLoss: 0.082352\n",
            "Epoch: 2 [Training: 35%]\tLoss: 0.106077\n",
            "Epoch: 2 [Training: 38%]\tLoss: 0.121010\n",
            "Epoch: 2 [Training: 42%]\tLoss: 0.168696\n",
            "Epoch: 2 [Training: 45%]\tLoss: 0.157291\n",
            "Epoch: 2 [Training: 48%]\tLoss: 0.118118\n",
            "Epoch: 2 [Training: 51%]\tLoss: 0.144452\n",
            "Epoch: 2 [Training: 54%]\tLoss: 0.150970\n",
            "Epoch: 2 [Training: 58%]\tLoss: 0.088300\n",
            "Epoch: 2 [Training: 61%]\tLoss: 0.031820\n",
            "Epoch: 2 [Training: 64%]\tLoss: 0.184265\n",
            "Epoch: 2 [Training: 67%]\tLoss: 0.126246\n",
            "Epoch: 2 [Training: 70%]\tLoss: 0.027296\n",
            "Epoch: 2 [Training: 74%]\tLoss: 0.275828\n",
            "Epoch: 2 [Training: 77%]\tLoss: 0.075901\n",
            "Epoch: 2 [Training: 80%]\tLoss: 0.063219\n",
            "Epoch: 2 [Training: 83%]\tLoss: 0.138453\n",
            "Epoch: 2 [Training: 86%]\tLoss: 0.062951\n",
            "Epoch: 2 [Training: 90%]\tLoss: 0.127558\n",
            "Epoch: 2 [Training: 93%]\tLoss: 0.167197\n",
            "Epoch: 2 [Training: 96%]\tLoss: 0.159204\n",
            "Epoch: 2 [Training: 99%]\tLoss: 0.124851\n",
            "\n",
            "Test set: Average loss: 0.0901, Accuracy: 9731/10000 (97%)\n",
            "\n",
            "Epoch: 3 [Training: 0%]\tLoss: 0.095849\n",
            "Epoch: 3 [Training: 3%]\tLoss: 0.192323\n",
            "Epoch: 3 [Training: 6%]\tLoss: 0.124923\n",
            "Epoch: 3 [Training: 10%]\tLoss: 0.089808\n",
            "Epoch: 3 [Training: 13%]\tLoss: 0.058414\n",
            "Epoch: 3 [Training: 16%]\tLoss: 0.175776\n",
            "Epoch: 3 [Training: 19%]\tLoss: 0.039286\n",
            "Epoch: 3 [Training: 22%]\tLoss: 0.083128\n",
            "Epoch: 3 [Training: 26%]\tLoss: 0.108054\n",
            "Epoch: 3 [Training: 29%]\tLoss: 0.101166\n",
            "Epoch: 3 [Training: 32%]\tLoss: 0.049796\n",
            "Epoch: 3 [Training: 35%]\tLoss: 0.054480\n",
            "Epoch: 3 [Training: 38%]\tLoss: 0.068002\n",
            "Epoch: 3 [Training: 42%]\tLoss: 0.059674\n",
            "Epoch: 3 [Training: 45%]\tLoss: 0.065851\n",
            "Epoch: 3 [Training: 48%]\tLoss: 0.065222\n",
            "Epoch: 3 [Training: 51%]\tLoss: 0.027976\n",
            "Epoch: 3 [Training: 54%]\tLoss: 0.020697\n",
            "Epoch: 3 [Training: 58%]\tLoss: 0.099166\n",
            "Epoch: 3 [Training: 61%]\tLoss: 0.095000\n",
            "Epoch: 3 [Training: 64%]\tLoss: 0.061594\n",
            "Epoch: 3 [Training: 67%]\tLoss: 0.041668\n",
            "Epoch: 3 [Training: 70%]\tLoss: 0.042193\n",
            "Epoch: 3 [Training: 74%]\tLoss: 0.144243\n",
            "Epoch: 3 [Training: 77%]\tLoss: 0.080398\n",
            "Epoch: 3 [Training: 80%]\tLoss: 0.035305\n",
            "Epoch: 3 [Training: 83%]\tLoss: 0.160486\n",
            "Epoch: 3 [Training: 86%]\tLoss: 0.064808\n",
            "Epoch: 3 [Training: 90%]\tLoss: 0.023737\n",
            "Epoch: 3 [Training: 93%]\tLoss: 0.065515\n",
            "Epoch: 3 [Training: 96%]\tLoss: 0.043922\n",
            "Epoch: 3 [Training: 99%]\tLoss: 0.034431\n",
            "\n",
            "Test set: Average loss: 0.0692, Accuracy: 9783/10000 (98%)\n",
            "\n",
            "Epoch: 4 [Training: 0%]\tLoss: 0.305790\n",
            "Epoch: 4 [Training: 3%]\tLoss: 0.030616\n",
            "Epoch: 4 [Training: 6%]\tLoss: 0.139796\n",
            "Epoch: 4 [Training: 10%]\tLoss: 0.044688\n",
            "Epoch: 4 [Training: 13%]\tLoss: 0.067052\n",
            "Epoch: 4 [Training: 16%]\tLoss: 0.042751\n",
            "Epoch: 4 [Training: 19%]\tLoss: 0.050923\n",
            "Epoch: 4 [Training: 22%]\tLoss: 0.026444\n",
            "Epoch: 4 [Training: 26%]\tLoss: 0.090016\n",
            "Epoch: 4 [Training: 29%]\tLoss: 0.058941\n",
            "Epoch: 4 [Training: 32%]\tLoss: 0.019156\n",
            "Epoch: 4 [Training: 35%]\tLoss: 0.085696\n",
            "Epoch: 4 [Training: 38%]\tLoss: 0.100230\n",
            "Epoch: 4 [Training: 42%]\tLoss: 0.343754\n",
            "Epoch: 4 [Training: 45%]\tLoss: 0.112247\n",
            "Epoch: 4 [Training: 48%]\tLoss: 0.083806\n",
            "Epoch: 4 [Training: 51%]\tLoss: 0.005163\n",
            "Epoch: 4 [Training: 54%]\tLoss: 0.020780\n",
            "Epoch: 4 [Training: 58%]\tLoss: 0.015707\n",
            "Epoch: 4 [Training: 61%]\tLoss: 0.125837\n",
            "Epoch: 4 [Training: 64%]\tLoss: 0.046638\n",
            "Epoch: 4 [Training: 67%]\tLoss: 0.034433\n",
            "Epoch: 4 [Training: 70%]\tLoss: 0.077820\n",
            "Epoch: 4 [Training: 74%]\tLoss: 0.022285\n",
            "Epoch: 4 [Training: 77%]\tLoss: 0.017292\n",
            "Epoch: 4 [Training: 80%]\tLoss: 0.074479\n",
            "Epoch: 4 [Training: 83%]\tLoss: 0.056607\n",
            "Epoch: 4 [Training: 86%]\tLoss: 0.122908\n",
            "Epoch: 4 [Training: 90%]\tLoss: 0.019609\n",
            "Epoch: 4 [Training: 93%]\tLoss: 0.066183\n",
            "Epoch: 4 [Training: 96%]\tLoss: 0.131226\n",
            "Epoch: 4 [Training: 99%]\tLoss: 0.047689\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 9828/10000 (98%)\n",
            "\n",
            "Epoch: 5 [Training: 0%]\tLoss: 0.056667\n",
            "Epoch: 5 [Training: 3%]\tLoss: 0.048501\n",
            "Epoch: 5 [Training: 6%]\tLoss: 0.078291\n",
            "Epoch: 5 [Training: 10%]\tLoss: 0.016656\n",
            "Epoch: 5 [Training: 13%]\tLoss: 0.072298\n",
            "Epoch: 5 [Training: 16%]\tLoss: 0.079415\n",
            "Epoch: 5 [Training: 19%]\tLoss: 0.116328\n",
            "Epoch: 5 [Training: 22%]\tLoss: 0.056774\n",
            "Epoch: 5 [Training: 26%]\tLoss: 0.048892\n",
            "Epoch: 5 [Training: 29%]\tLoss: 0.005368\n",
            "Epoch: 5 [Training: 32%]\tLoss: 0.096708\n",
            "Epoch: 5 [Training: 35%]\tLoss: 0.056304\n",
            "Epoch: 5 [Training: 38%]\tLoss: 0.013416\n",
            "Epoch: 5 [Training: 42%]\tLoss: 0.062279\n",
            "Epoch: 5 [Training: 45%]\tLoss: 0.070761\n",
            "Epoch: 5 [Training: 48%]\tLoss: 0.026311\n",
            "Epoch: 5 [Training: 51%]\tLoss: 0.068014\n",
            "Epoch: 5 [Training: 54%]\tLoss: 0.060263\n",
            "Epoch: 5 [Training: 58%]\tLoss: 0.030376\n",
            "Epoch: 5 [Training: 61%]\tLoss: 0.037922\n",
            "Epoch: 5 [Training: 64%]\tLoss: 0.132276\n",
            "Epoch: 5 [Training: 67%]\tLoss: 0.016309\n",
            "Epoch: 5 [Training: 70%]\tLoss: 0.057910\n",
            "Epoch: 5 [Training: 74%]\tLoss: 0.041448\n",
            "Epoch: 5 [Training: 77%]\tLoss: 0.060802\n",
            "Epoch: 5 [Training: 80%]\tLoss: 0.076934\n",
            "Epoch: 5 [Training: 83%]\tLoss: 0.046458\n",
            "Epoch: 5 [Training: 86%]\tLoss: 0.027223\n",
            "Epoch: 5 [Training: 90%]\tLoss: 0.087364\n",
            "Epoch: 5 [Training: 93%]\tLoss: 0.059614\n",
            "Epoch: 5 [Training: 96%]\tLoss: 0.028582\n",
            "Epoch: 5 [Training: 99%]\tLoss: 0.020494\n",
            "\n",
            "Test set: Average loss: 0.0466, Accuracy: 9854/10000 (99%)\n",
            "\n",
            "Epoch: 6 [Training: 0%]\tLoss: 0.051336\n",
            "Epoch: 6 [Training: 3%]\tLoss: 0.008204\n",
            "Epoch: 6 [Training: 6%]\tLoss: 0.073106\n",
            "Epoch: 6 [Training: 10%]\tLoss: 0.139433\n",
            "Epoch: 6 [Training: 13%]\tLoss: 0.026254\n",
            "Epoch: 6 [Training: 16%]\tLoss: 0.070038\n",
            "Epoch: 6 [Training: 19%]\tLoss: 0.069010\n",
            "Epoch: 6 [Training: 22%]\tLoss: 0.006482\n",
            "Epoch: 6 [Training: 26%]\tLoss: 0.036584\n",
            "Epoch: 6 [Training: 29%]\tLoss: 0.032643\n",
            "Epoch: 6 [Training: 32%]\tLoss: 0.099757\n",
            "Epoch: 6 [Training: 35%]\tLoss: 0.040004\n",
            "Epoch: 6 [Training: 38%]\tLoss: 0.089557\n",
            "Epoch: 6 [Training: 42%]\tLoss: 0.148698\n",
            "Epoch: 6 [Training: 45%]\tLoss: 0.030818\n",
            "Epoch: 6 [Training: 48%]\tLoss: 0.023862\n",
            "Epoch: 6 [Training: 51%]\tLoss: 0.089148\n",
            "Epoch: 6 [Training: 54%]\tLoss: 0.020187\n",
            "Epoch: 6 [Training: 58%]\tLoss: 0.038715\n",
            "Epoch: 6 [Training: 61%]\tLoss: 0.179962\n",
            "Epoch: 6 [Training: 64%]\tLoss: 0.157441\n",
            "Epoch: 6 [Training: 67%]\tLoss: 0.005985\n",
            "Epoch: 6 [Training: 70%]\tLoss: 0.056749\n",
            "Epoch: 6 [Training: 74%]\tLoss: 0.017910\n",
            "Epoch: 6 [Training: 77%]\tLoss: 0.017312\n",
            "Epoch: 6 [Training: 80%]\tLoss: 0.081378\n",
            "Epoch: 6 [Training: 83%]\tLoss: 0.042159\n",
            "Epoch: 6 [Training: 86%]\tLoss: 0.029318\n",
            "Epoch: 6 [Training: 90%]\tLoss: 0.076899\n",
            "Epoch: 6 [Training: 93%]\tLoss: 0.060386\n",
            "Epoch: 6 [Training: 96%]\tLoss: 0.061302\n",
            "Epoch: 6 [Training: 99%]\tLoss: 0.024381\n",
            "\n",
            "Test set: Average loss: 0.0424, Accuracy: 9882/10000 (99%)\n",
            "\n",
            "Epoch: 7 [Training: 0%]\tLoss: 0.048324\n",
            "Epoch: 7 [Training: 3%]\tLoss: 0.037897\n",
            "Epoch: 7 [Training: 6%]\tLoss: 0.067680\n",
            "Epoch: 7 [Training: 10%]\tLoss: 0.029495\n",
            "Epoch: 7 [Training: 13%]\tLoss: 0.012459\n",
            "Epoch: 7 [Training: 16%]\tLoss: 0.057764\n",
            "Epoch: 7 [Training: 19%]\tLoss: 0.079778\n",
            "Epoch: 7 [Training: 22%]\tLoss: 0.011568\n",
            "Epoch: 7 [Training: 26%]\tLoss: 0.037144\n",
            "Epoch: 7 [Training: 29%]\tLoss: 0.023196\n",
            "Epoch: 7 [Training: 32%]\tLoss: 0.018863\n",
            "Epoch: 7 [Training: 35%]\tLoss: 0.258291\n",
            "Epoch: 7 [Training: 38%]\tLoss: 0.024505\n",
            "Epoch: 7 [Training: 42%]\tLoss: 0.043402\n",
            "Epoch: 7 [Training: 45%]\tLoss: 0.066417\n",
            "Epoch: 7 [Training: 48%]\tLoss: 0.039265\n",
            "Epoch: 7 [Training: 51%]\tLoss: 0.088995\n",
            "Epoch: 7 [Training: 54%]\tLoss: 0.170048\n",
            "Epoch: 7 [Training: 58%]\tLoss: 0.004378\n",
            "Epoch: 7 [Training: 61%]\tLoss: 0.039968\n",
            "Epoch: 7 [Training: 64%]\tLoss: 0.013079\n",
            "Epoch: 7 [Training: 67%]\tLoss: 0.014314\n",
            "Epoch: 7 [Training: 70%]\tLoss: 0.020096\n",
            "Epoch: 7 [Training: 74%]\tLoss: 0.033182\n",
            "Epoch: 7 [Training: 77%]\tLoss: 0.013349\n",
            "Epoch: 7 [Training: 80%]\tLoss: 0.023704\n",
            "Epoch: 7 [Training: 83%]\tLoss: 0.044400\n",
            "Epoch: 7 [Training: 86%]\tLoss: 0.032545\n",
            "Epoch: 7 [Training: 90%]\tLoss: 0.058582\n",
            "Epoch: 7 [Training: 93%]\tLoss: 0.074350\n",
            "Epoch: 7 [Training: 96%]\tLoss: 0.010761\n",
            "Epoch: 7 [Training: 99%]\tLoss: 0.011016\n",
            "\n",
            "Test set: Average loss: 0.0443, Accuracy: 9864/10000 (99%)\n",
            "\n",
            "Epoch: 8 [Training: 0%]\tLoss: 0.002769\n",
            "Epoch: 8 [Training: 3%]\tLoss: 0.097144\n",
            "Epoch: 8 [Training: 6%]\tLoss: 0.016948\n",
            "Epoch: 8 [Training: 10%]\tLoss: 0.012964\n",
            "Epoch: 8 [Training: 13%]\tLoss: 0.040745\n",
            "Epoch: 8 [Training: 16%]\tLoss: 0.006641\n",
            "Epoch: 8 [Training: 19%]\tLoss: 0.034673\n",
            "Epoch: 8 [Training: 22%]\tLoss: 0.019562\n",
            "Epoch: 8 [Training: 26%]\tLoss: 0.022602\n",
            "Epoch: 8 [Training: 29%]\tLoss: 0.016983\n",
            "Epoch: 8 [Training: 32%]\tLoss: 0.013304\n",
            "Epoch: 8 [Training: 35%]\tLoss: 0.041412\n",
            "Epoch: 8 [Training: 38%]\tLoss: 0.013117\n",
            "Epoch: 8 [Training: 42%]\tLoss: 0.027847\n",
            "Epoch: 8 [Training: 45%]\tLoss: 0.060251\n",
            "Epoch: 8 [Training: 48%]\tLoss: 0.058194\n",
            "Epoch: 8 [Training: 51%]\tLoss: 0.119992\n",
            "Epoch: 8 [Training: 54%]\tLoss: 0.028934\n",
            "Epoch: 8 [Training: 58%]\tLoss: 0.032972\n",
            "Epoch: 8 [Training: 61%]\tLoss: 0.027532\n",
            "Epoch: 8 [Training: 64%]\tLoss: 0.004476\n",
            "Epoch: 8 [Training: 67%]\tLoss: 0.047269\n",
            "Epoch: 8 [Training: 70%]\tLoss: 0.007759\n",
            "Epoch: 8 [Training: 74%]\tLoss: 0.013600\n",
            "Epoch: 8 [Training: 77%]\tLoss: 0.034864\n",
            "Epoch: 8 [Training: 80%]\tLoss: 0.007232\n",
            "Epoch: 8 [Training: 83%]\tLoss: 0.063252\n",
            "Epoch: 8 [Training: 86%]\tLoss: 0.005525\n",
            "Epoch: 8 [Training: 90%]\tLoss: 0.062958\n",
            "Epoch: 8 [Training: 93%]\tLoss: 0.107900\n",
            "Epoch: 8 [Training: 96%]\tLoss: 0.042711\n",
            "Epoch: 8 [Training: 99%]\tLoss: 0.083873\n",
            "\n",
            "Test set: Average loss: 0.0397, Accuracy: 9876/10000 (99%)\n",
            "\n",
            "Epoch: 9 [Training: 0%]\tLoss: 0.022293\n",
            "Epoch: 9 [Training: 3%]\tLoss: 0.101694\n",
            "Epoch: 9 [Training: 6%]\tLoss: 0.031540\n",
            "Epoch: 9 [Training: 10%]\tLoss: 0.048125\n",
            "Epoch: 9 [Training: 13%]\tLoss: 0.018222\n",
            "Epoch: 9 [Training: 16%]\tLoss: 0.049689\n",
            "Epoch: 9 [Training: 19%]\tLoss: 0.030312\n",
            "Epoch: 9 [Training: 22%]\tLoss: 0.111611\n",
            "Epoch: 9 [Training: 26%]\tLoss: 0.035983\n",
            "Epoch: 9 [Training: 29%]\tLoss: 0.009334\n",
            "Epoch: 9 [Training: 32%]\tLoss: 0.187581\n",
            "Epoch: 9 [Training: 35%]\tLoss: 0.067450\n",
            "Epoch: 9 [Training: 38%]\tLoss: 0.066732\n",
            "Epoch: 9 [Training: 42%]\tLoss: 0.012219\n",
            "Epoch: 9 [Training: 45%]\tLoss: 0.010875\n",
            "Epoch: 9 [Training: 48%]\tLoss: 0.012800\n",
            "Epoch: 9 [Training: 51%]\tLoss: 0.013669\n",
            "Epoch: 9 [Training: 54%]\tLoss: 0.075776\n",
            "Epoch: 9 [Training: 58%]\tLoss: 0.007088\n",
            "Epoch: 9 [Training: 61%]\tLoss: 0.012529\n",
            "Epoch: 9 [Training: 64%]\tLoss: 0.025874\n",
            "Epoch: 9 [Training: 67%]\tLoss: 0.025486\n",
            "Epoch: 9 [Training: 70%]\tLoss: 0.073442\n",
            "Epoch: 9 [Training: 74%]\tLoss: 0.002903\n",
            "Epoch: 9 [Training: 77%]\tLoss: 0.064342\n",
            "Epoch: 9 [Training: 80%]\tLoss: 0.031560\n",
            "Epoch: 9 [Training: 83%]\tLoss: 0.077909\n",
            "Epoch: 9 [Training: 86%]\tLoss: 0.006844\n",
            "Epoch: 9 [Training: 90%]\tLoss: 0.054261\n",
            "Epoch: 9 [Training: 93%]\tLoss: 0.022980\n",
            "Epoch: 9 [Training: 96%]\tLoss: 0.043786\n",
            "Epoch: 9 [Training: 99%]\tLoss: 0.078632\n",
            "\n",
            "Test set: Average loss: 0.0398, Accuracy: 9880/10000 (99%)\n",
            "\n",
            "Epoch: 10 [Training: 0%]\tLoss: 0.008845\n",
            "Epoch: 10 [Training: 3%]\tLoss: 0.008479\n",
            "Epoch: 10 [Training: 6%]\tLoss: 0.008885\n",
            "Epoch: 10 [Training: 10%]\tLoss: 0.148903\n",
            "Epoch: 10 [Training: 13%]\tLoss: 0.026750\n",
            "Epoch: 10 [Training: 16%]\tLoss: 0.032019\n",
            "Epoch: 10 [Training: 19%]\tLoss: 0.143783\n",
            "Epoch: 10 [Training: 22%]\tLoss: 0.119613\n",
            "Epoch: 10 [Training: 26%]\tLoss: 0.009525\n",
            "Epoch: 10 [Training: 29%]\tLoss: 0.021028\n",
            "Epoch: 10 [Training: 32%]\tLoss: 0.008773\n",
            "Epoch: 10 [Training: 35%]\tLoss: 0.117881\n",
            "Epoch: 10 [Training: 38%]\tLoss: 0.011637\n",
            "Epoch: 10 [Training: 42%]\tLoss: 0.015059\n",
            "Epoch: 10 [Training: 45%]\tLoss: 0.011796\n",
            "Epoch: 10 [Training: 48%]\tLoss: 0.034562\n",
            "Epoch: 10 [Training: 51%]\tLoss: 0.034041\n",
            "Epoch: 10 [Training: 54%]\tLoss: 0.013119\n",
            "Epoch: 10 [Training: 58%]\tLoss: 0.084798\n",
            "Epoch: 10 [Training: 61%]\tLoss: 0.023818\n",
            "Epoch: 10 [Training: 64%]\tLoss: 0.010749\n",
            "Epoch: 10 [Training: 67%]\tLoss: 0.098743\n",
            "Epoch: 10 [Training: 70%]\tLoss: 0.029088\n",
            "Epoch: 10 [Training: 74%]\tLoss: 0.008574\n",
            "Epoch: 10 [Training: 77%]\tLoss: 0.027983\n",
            "Epoch: 10 [Training: 80%]\tLoss: 0.046946\n",
            "Epoch: 10 [Training: 83%]\tLoss: 0.087109\n",
            "Epoch: 10 [Training: 86%]\tLoss: 0.015680\n",
            "Epoch: 10 [Training: 90%]\tLoss: 0.030113\n",
            "Epoch: 10 [Training: 93%]\tLoss: 0.019619\n",
            "Epoch: 10 [Training: 96%]\tLoss: 0.004242\n",
            "Epoch: 10 [Training: 99%]\tLoss: 0.007204\n",
            "\n",
            "Test set: Average loss: 0.0335, Accuracy: 9893/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2MKjHouu3dr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "10e4eb56-e29f-4702-c0ec-6b7758393bd7"
      },
      "source": [
        "print(\"Accuracy Obtained {:.4f}%\".format( 100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy Obtained 98.9300%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}